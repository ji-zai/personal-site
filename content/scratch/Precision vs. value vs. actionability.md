I have a problem in the model: how is a child want generated?

There is not a high enough value truth to fulfill the want (i.e. say, the topmost truth in the model doesn't have the precision required to fulfill your want), then why would you even dig deeper? This makes no sense, if you confidently dig deeper, with some confidence you must believe it will fulfill your want, else you wouldn't do it. Therefore, a truth with high enough value, and some confidence must exist for you to dig into it.

OK, what does this mean?

The highest value truth has a high value, but low confidence (bec. of course you're not entirely sure) - nope, this is wrong, because for ordering food the following truth is incredibly high confidence: "I can order food to get what I want by opening Doordash". This is near perfect confidence. The only thing lacking in this statement is the required precision to be actionable, actually no, it is perfectly actionable, it just doesn't take you all the way. Once you open doordash, the want changes from get food to: find the best dish to order.

Let's use a better example: building a tech product:
- I want to solve this particular problem that I have, and I predict multiple solutions.