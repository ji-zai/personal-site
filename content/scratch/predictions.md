All of the predictions will be in play until we have AGI at the level of intelligence of a human being. (actually we need to define this quite precisely).

AND: this also needs to establish that AGI will be the final frontier of technology. And that I believe it is quite far away. Progress will initially come fast, but the final goal is much farther away than expected. Both are true. - and this is another such paradoxical-esque kind of thinking, but I'm confident it tends towards the truth. This means that anyone who believes either is going to be deluded to the degree they believe either extreme.

- Acting vs. processing (no acting)
- Secrecy vs. transparency
- Hierarchical vs. flat organizations
  - We will also be tending towards equal compensation for all employees - and this is an interesting point that seems contradictory from the outside. But I think this is also true.
- Specialist vs. generalist.
- Competition vs. collaboration.
- Centralized vs. decentralized
- Equality vs. extreme power law (towards extreme)
- Many vs. few wants (towards fewer)
- Gradual progress vs. hit driven
- Credential based status (or credibility) vs. results based status.
- Understanding the universe vs. understanding the human condition.
  - Show how programmers evolve to understanding what people want.
  - The best chess players have graduated to creating content - i.e. solving for the human wants.
- Teaching truths vs. teaching seeking truth
  - In the past, individual truths were incredibly valuable. Hence our education system is more about communicating facts rather than training the process through which you can seek truth. As we approach the limit, the only education that matters will explain the human condition to a child (according to their age, teach them the minimal communication technique to transmit that information to their minds - whether that is English or not we'll see), protect them from themselves until their mind has matured (a rough heuristic could be 25 years - but most likely this will depend on the child) (i.e. protect them from harmful addictions that prevent them from seeking truth), and let them do whatever they want. Any other centralized, opinionate education will be sub-optimal.
- Low ambition to high ambition
- Short-term thinking to long-term thinking
- Low ego to high ego (this one is a bit problematic unless you really, really explain it).
- Less humility to greater humility
- Long research winters to no winters ()

And even current AI research is more about understanding the human condition than it is about understanding the universe. As the wise for millenia have said: you can understand the universe by understanding yourself.

And these do not follow the same curve. I.e. we are much further ahead on flat vs. hierarchical for example.

They are completely dependent (shift in one implies a shift in all others) and simply different manifestations of the same change. Different ways of expressing the same change is useful because it allows us to predict in different aspects of our lives and society. I.e. if you were to mathematically represent these changes from the model of seeking truth, then all of these predictions are directly derived.

(I also provide some notes on how we can mathematically represent them. But, I don't know the mathematical tools, etc. to do so, nor do I think learning them is a good use of my time as others already possess that ability. Therefore I request folks with that background to help me!).

The optimal will jump towards the limit quickly and then slow down. I don't know the curve of that, but I'll of course need to predict this in order to write point 4. (optimal when gpt-5 comes out).

For each dimension:

1. The limit we're moving towards.
2. Evidence we are moving towards this limit.
3. What is optimal today.
4. What will be optimal when GPT-5 releases and it is twice as capable as today's cutting edge LLM.

There is another important dimension here: individual, company, nation.

- the advice that when you find your optimal, don't adjust until the next step function.

When explaining what is optimal right now, you have to explain why they all follow the principle of "more than you think you should" - because of the implications of outdated conditioning. Amen.
